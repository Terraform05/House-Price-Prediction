{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from tqdm import tqdm\n",
    "\n",
    "#models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = pd.read_csv('../data/train.csv')\n",
    "numeric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric_df.drop(['Id'], axis=1, inplace=True)\n",
    "#numeric_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = numeric_df.select_dtypes(include=[np.number])\n",
    "print(numeric_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(numeric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating LotFrontage\n",
    "fill in NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LotFrontage_numeric_df = numeric_df.dropna(subset=['LotFrontage'])\n",
    "LotFrontage_numeric_df.corr()['LotFrontage'].sort_values(ascending=False)[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajr_plot_scatter_volume(LotFrontage_numeric_df, 'LotArea', 'LotFrontage', fig_height=8)\n",
    "\n",
    "def iqf(data: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    q1 = data[column].quantile(0.25)\n",
    "    q3 = data[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return data[(data[column] > lower_bound) & (data[column] < upper_bound)]\n",
    "\n",
    "LotFrontage_numeric_df = iqf(LotFrontage_numeric_df, 'LotArea')\n",
    "\n",
    "ajr_plot_scatter_volume(LotFrontage_numeric_df, 'LotArea', 'LotFrontage', fig_height=8)\n",
    "ajr_plot_scatter_volume(LotFrontage_numeric_df, '1stFlrSF', 'LotFrontage', fig_height=8)\n",
    "#ajr_plot_scatter_volume(LotFrontage_numeric_df, 'GrLivArea', 'LotFrontage', fig_height=8)\n",
    "#ajr_plot_scatter_volume(LotFrontage_numeric_df, 'TotalBsmtSF', 'LotFrontage', fig_height=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LotFrontage_top_features = numeric_df.corr()['LotFrontage'].sort_values(ascending=False)[1:5].index.to_list()\n",
    "LotFrontage_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression(), Ridge(), Lasso(), ElasticNet(), GradientBoostingRegressor(), AdaBoostRegressor(), BaggingRegressor(), ExtraTreesRegressor(), DecisionTreeRegressor(), KNeighborsRegressor(), SVR()]\n",
    "\n",
    "scores = {}\n",
    "for i in tqdm(range(20)):\n",
    "    for model in models:\n",
    "        X = LotFrontage_numeric_df[['LotArea', '1stFlrSF', 'GrLivArea', 'TotalBsmtSF']]\n",
    "        y = LotFrontage_numeric_df['LotFrontage']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = r2_score(y_test, y_pred)\n",
    "        if model in scores:\n",
    "            scores[model].append(score)\n",
    "        else:\n",
    "            scores[model] = [score]\n",
    "\n",
    "# Calculate the mean score for each model\n",
    "mean_scores = {model: np.mean(score_list) for model, score_list in scores.items()}\n",
    "\n",
    "# Find the model with the highest mean score\n",
    "best_model = max(mean_scores, key=mean_scores.get)\n",
    "\n",
    "# Print the mean scores for each model\n",
    "for model, mean_score in mean_scores.items():\n",
    "    print(f'{model}: {mean_score}')\n",
    "\n",
    "best_model, mean_scores[best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lot_frontage_model = SVR()\n",
    "X = LotFrontage_numeric_df[['1stFlrSF', 'LotArea', 'GrLivArea', 'TotalBsmtSF']]\n",
    "y = LotFrontage_numeric_df['LotFrontage']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lot_frontage_model.fit(X_train, y_train)\n",
    "y_pred = lot_frontage_model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'R2: {r2}')\n",
    "print(f'MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_lotfrontage_indices = numeric_df[numeric_df['LotFrontage'].isnull()].index\n",
    "missing_features = numeric_df.loc[missing_lotfrontage_indices, ['1stFlrSF', 'LotArea', 'GrLivArea', 'TotalBsmtSF']]\n",
    "predicted_lotfrontage = lot_frontage_model.predict(missing_features)\n",
    "numeric_df.loc[missing_lotfrontage_indices,'LotFrontage'] = predicted_lotfrontage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested with various regression models and combinations of features to predict LotFrontage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LotFrontage is the linear feet of street connected to the property. It is a continuous variable. It is reasonable to assume that the LotFrontage of a property is related to the LotArea of the property. We can use this relationship to estimate the LotFrontage of properties with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GarageYrBlt is null because they don't have a garage. We can replace these null values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(numeric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MasVnrArea\n",
    "fill in NA values. These null values are due to the fact that the property does not have a veneer, leading to null values. No veneer also means square footage of veneer is 0, which is why we can replace these null values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df['MasVnrArea'] = numeric_df['MasVnrArea'].fillna(0)\n",
    "numeric_df['GarageYrBlt'] = numeric_df['GarageYrBlt'].fillna(0)\n",
    "\n",
    "count_nulls(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.to_csv('../new_data/clean_train_numeric.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GarageYrBlt\n",
    "These null values are due to the fact that the property does not have a garage, leading to null values. \n",
    "There is no reasonable replacement since the year the garage was built is not available. These rows must be dropped for regression but kept for other classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric_df = numeric_df.dropna(subset=['GarageYrBlt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajr_plot_histograms(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajr_plot_correlations(numeric_df, 'SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_corr = numeric_df.corr()\n",
    "\n",
    "def ajr_correlation_plot(corr_numeric_df, fig_height=10):\n",
    "    column_names = list(corr_numeric_df.columns.values)\n",
    "    plt.figure(figsize=(fig_height, fig_height))\n",
    "    plt.matshow(corr_numeric_df, cmap='coolwarm', fignum=1)\n",
    "    plt.xticks(np.arange(len(column_names)), column_names, rotation=90)\n",
    "    plt.yticks(np.arange(len(column_names)), column_names)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    for i in range(len(corr_numeric_df.columns)):\n",
    "        for j in range(len(corr_numeric_df.columns)):\n",
    "            plt.text(j, i, f'{round(corr_numeric_df.iloc[i, j], 2)}', ha='center', va='center', color='black')\n",
    "\n",
    "ajr_correlation_plot(numeric_corr, fig_height=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model using numerical only features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only the features that have a correlation of 0.5 or higher with SalePrice\n",
    "numeric_corr = numeric_corr['SalePrice']\n",
    "numeric_corr = numeric_corr[numeric_corr > 0.5]\n",
    "\n",
    "ajr_correlation_plot(numeric_df[numeric_corr.index].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numeric_df.drop(columns=['SalePrice'])\n",
    "y = numeric_df['SalePrice']\n",
    "\n",
    "correlation_matrix = numeric_df.corr()\n",
    "correlated_features = correlation_matrix['SalePrice'][abs(correlation_matrix['SalePrice']) > 0.5].index\n",
    "X = X[correlated_features.drop('SalePrice')]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Ridge': {'alpha': [0.1, 1, 10, 100]},\n",
    "    'Lasso': {'alpha': [0.01, 0.1, 1, 10]},\n",
    "    'ElasticNet': {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.2, 0.5, 0.8]},\n",
    "    'RandomForestRegressor': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'GradientBoostingRegressor': {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200]},\n",
    "    'AdaBoostRegressor': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 1]},\n",
    "    'BaggingRegressor': {'n_estimators': [10, 50, 100]},\n",
    "    'ExtraTreesRegressor': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'DecisionTreeRegressor': {'max_depth': [None, 10, 20]},\n",
    "    'KNeighborsRegressor': {'n_neighbors': [3, 5, 10]},\n",
    "    'SVR': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "    'BaggingRegressor': BaggingRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'SVR': SVR(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "print('Training models... tuning hyperparameters...')\n",
    "for name, model in tqdm(models.items()):\n",
    "    grid_search = GridSearchCV(model, param_grids.get(name, {}), cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_rmse = np.sqrt(-grid_search.best_score_)  # RMSE\n",
    "\n",
    "    results[name] = best_rmse\n",
    "    best_models[name] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nModel Performance (RMSE):')\n",
    "for name, rmse in results.items():\n",
    "    print(f'{name}: {rmse:.4f}')\n",
    "\n",
    "print('\\nBest Model:')\n",
    "best_model = best_models[min(results, key=results.get)]\n",
    "print(f'{best_model}: {results[min(results, key=results.get)]:.4f}')\n",
    "\n",
    "print(best_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = zip(X.columns, best_model.feature_importances_)\n",
    "feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, importance in feature_importances:\n",
    "    print(f'{feature}: {importance:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    colors = cm.viridis([x[1] for x in feature_importances])\n",
    "    plt.barh([x[0] for x in feature_importances], [x[1] for x in feature_importances], color=colors)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    for i, (feature, importance) in enumerate(feature_importances):\n",
    "        plt.text(importance - 0.001, i, f'{importance:.4f}', ha='right', va='center', color='white')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    MDNA SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pca= PCA(n_components=10)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(f'Total explained variance with selected components: {explained_variance[-1]:.4f}')\n",
    "print(f'Number of components: {pca.n_components_}')\n",
    "\n",
    "param_grids = {\n",
    "    'Ridge': {'alpha': [0.1, 1, 10, 100]},\n",
    "    'Lasso': {'alpha': [0.01, 0.1, 1, 10]},\n",
    "    'ElasticNet': {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.2, 0.5, 0.8]},\n",
    "    'RandomForestRegressor': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'GradientBoostingRegressor': {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200]},\n",
    "    'AdaBoostRegressor': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 1]},\n",
    "    'BaggingRegressor': {'n_estimators': [10, 50, 100]},\n",
    "    'ExtraTreesRegressor': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'DecisionTreeRegressor': {'max_depth': [None, 10, 20]},\n",
    "    'KNeighborsRegressor': {'n_neighbors': [3, 5, 10]},\n",
    "    'SVR': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "    'BaggingRegressor': BaggingRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'SVR': SVR(),\n",
    "}\n",
    "\n",
    "results_pca = {}\n",
    "best_models_pca = {}\n",
    "\n",
    "print('Training models with PCA-transformed features...')\n",
    "for name, model in tqdm(models.items()):\n",
    "    grid_search = GridSearchCV(model, param_grids.get(name, {}), cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_rmse = np.sqrt(-grid_search.best_score_)  # RMSE\n",
    "\n",
    "    results_pca[name] = best_rmse\n",
    "    best_models_pca[name] = best_model\n",
    "\n",
    "print('\\nModel Performance with PCA (RMSE):')\n",
    "for name, rmse in results_pca.items():\n",
    "    print(f'{name}: {rmse:.4f}')\n",
    "\n",
    "print('\\nBest Model with PCA:')\n",
    "best_model_pca = best_models_pca[min(results_pca, key=results_pca.get)]\n",
    "print(f'{best_model_pca}: {results_pca[min(results_pca, key=results_pca.get)]:.4f}')\n",
    "\n",
    "print(best_model_pca.get_params())\n",
    "\n",
    "#visualize explained variance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(explained_variance, marker='o', label=\"Cumulative Explained Variance\")\n",
    "plt.axhline(y=explained_variance[-1], color='r', linestyle='--', label=f\"{explained_variance[-1]:.2f} variance explained\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Explained Variance by PCA Components\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "param_grid = {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
    "\n",
    "sfs = SequentialFeatureSelector(\n",
    "    model,\n",
    "    n_features_to_select='auto',\n",
    "    direction='forward',\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "X_train_sfs = sfs.transform(X_train)\n",
    "X_test_sfs = sfs.transform(X_test)\n",
    "\n",
    "#grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "#grid_search.fit(X_train_sfs, y_train)\n",
    "model.fit(X_train_sfs, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_sfs)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'r2: {r2_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the distribution of saleprice\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(y, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sale Price')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sale_price = y.min()\n",
    "max_sale_price = y.max()\n",
    "\n",
    "print(f'Min: {min_sale_price}, Max: {max_sale_price}, Range: {max_sale_price - min_sale_price}')\n",
    "\n",
    "#rmse as percent of sale range\n",
    "rmse_percent = rmse / (max_sale_price - min_sale_price) * 100\n",
    "print(f'RMSE as a percent of the range of Sale Prices: {rmse_percent:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
