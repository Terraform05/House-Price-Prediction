{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "#models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_df = df.select_dtypes(exclude=[np.number])\n",
    "print(categoric_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_df.drop(['MiscFeature'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'MiscFeature' is the feature of the house not covered in other categories but their is a feature called 'MiscVal' which is the value of the 'MiscFeature'. So, we can drop this feature as it is just a description but the value is already accounted for in numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(categoric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of nulls for Pools, Alleys, Fences, etc in the dataset. However in the description **NA** means No Pool, No Alley, No Fence, etc. So I will replace certain nulls with descriptions in the notes with (str) **'none'** before onehot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_pairings = {'PoolQC' : 'No Pool',\n",
    "               'Alley' : 'No Alley',\n",
    "                'Fence' : 'No Fence',\n",
    "                'FireplaceQu' : 'No Fireplace',\n",
    "                'GarageType' : 'No Garage',\n",
    "                'GarageFinish' : 'No Garage',\n",
    "                'GarageQual' : 'No Garage',\n",
    "                'GarageCond' : 'No Garage',\n",
    "                'BsmtExposure' : 'No Basement',\n",
    "                'BsmtFinType1' : 'No Basement',\n",
    "                'BsmtFinType2' : 'No Basement',\n",
    "                'BsmtQual' : 'No Basement',\n",
    "                'BsmtCond' : 'No Basement',        \n",
    "                }\n",
    "\n",
    "#electircal should not be na, drop na electorcal row\n",
    "categoric_df.dropna(subset=['Electrical'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove masonry veneer type col\n",
    "categoric_df.drop(['MasVnrType'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply one hot encoding\n",
    "categoric_df = pd.get_dummies(categoric_df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(categoric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "categoric_df['Id'] = df['Id']\n",
    "categoric_df.to_csv('../new_data/clean_train_categoric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_df.drop(['Id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model using categorical only features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = categoric_df.drop(columns=['SalePrice'])\n",
    "y = categoric_df['SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "base_model = RandomForestRegressor(random_state=42) \n",
    "rfe = RFE(estimator=base_model, n_features_to_select=20, verbose=2)\n",
    "\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "param_grids = {\n",
    "    'Ridge': {'alpha': [0.1, 1, 10, 100]},\n",
    "    'Lasso': {'alpha': [0.01, 0.1, 1, 10]},\n",
    "    'ElasticNet': {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.2, 0.5, 0.8]},\n",
    "    'RandomForestRegressor': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'GradientBoostingRegressor': {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200]},\n",
    "    'AdaBoostRegressor': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 1]},\n",
    "    'BaggingRegressor': {'n_estimators': [10, 50, 100]},\n",
    "    'ExtraTreesRegressor': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'DecisionTreeRegressor': {'max_depth': [None, 10, 20]},\n",
    "    'KNeighborsRegressor': {'n_neighbors': [3, 5, 10]},\n",
    "    'SVR': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "    'BaggingRegressor': BaggingRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'SVR': SVR(),\n",
    "}\n",
    "\n",
    "results_rfe = {}\n",
    "best_models_rfe = {}\n",
    "\n",
    "print('Training models with RFE-selected features...')\n",
    "for name, model in tqdm(models.items()):\n",
    "    grid_search = GridSearchCV(model, param_grids.get(name, {}), cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train_rfe, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_rmse = np.sqrt(-grid_search.best_score_)  # RMSE\n",
    "\n",
    "    results_rfe[name] = best_rmse\n",
    "    best_models_rfe[name] = best_model\n",
    "\n",
    "print('\\nModel Performance with RFE (RMSE):')\n",
    "for name, rmse in results_rfe.items():\n",
    "    print(f'{name}: {rmse:.4f}')\n",
    "\n",
    "print('\\nBest Model with RFE:')\n",
    "best_model_rfe = best_models_rfe[min(results_rfe, key=results_rfe.get)]\n",
    "print(f'{best_model_rfe}: {results_rfe[min(results_rfe, key=results_rfe.get)]:.4f}')\n",
    "\n",
    "print(best_model_rfe.get_params())\n",
    "\n",
    "if hasattr(best_model_rfe, \"feature_importances_\"):\n",
    "    feature_importances = zip(selected_features, best_model_rfe.feature_importances_)\n",
    "    feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for feature, importance in feature_importances:\n",
    "        print(f'{feature}: {importance:.4f}')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    import matplotlib.cm as cm\n",
    "    colors = cm.viridis([x[1] for x in feature_importances])\n",
    "    plt.barh([x[0] for x in feature_importances], [x[1] for x in feature_importances], color=colors)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importances (RFE)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for i, (feature, importance) in enumerate(feature_importances):\n",
    "        plt.text(importance - 0.001, i, f'{importance:.4f}', ha='right', va='center', color='white')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "X = categoric_df.drop(columns=['SalePrice'])\n",
    "y = categoric_df['SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(f\"Total explained variance with selected components: {explained_variance[-1]:.4f}\")\n",
    "print(f\"Number of components selected: {pca.n_components_}\")\n",
    "\n",
    "param_grids = {\n",
    "    'Ridge': {'alpha': [0.1, 1, 10, 100]},\n",
    "    'Lasso': {'alpha': [0.01, 0.1, 1, 10]},\n",
    "    'ElasticNet': {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.2, 0.5, 0.8]},\n",
    "    'RandomForestRegressor': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'GradientBoostingRegressor': {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200]},\n",
    "    'AdaBoostRegressor': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 1]},\n",
    "    'BaggingRegressor': {'n_estimators': [10, 50, 100]},\n",
    "    'ExtraTreesRegressor': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'DecisionTreeRegressor': {'max_depth': [None, 10, 20]},\n",
    "    'KNeighborsRegressor': {'n_neighbors': [3, 5, 10]},\n",
    "    'SVR': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "    'BaggingRegressor': BaggingRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'SVR': SVR(),\n",
    "}\n",
    "\n",
    "results_pca = {}\n",
    "best_models_pca = {}\n",
    "\n",
    "print('Training models with PCA-transformed features...')\n",
    "for name, model in tqdm(models.items()):\n",
    "    grid_search = GridSearchCV(model, param_grids.get(name, {}), cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_rmse = np.sqrt(-grid_search.best_score_)  # RMSE\n",
    "\n",
    "    results_pca[name] = best_rmse\n",
    "    best_models_pca[name] = best_model\n",
    "\n",
    "# Display results\n",
    "print('\\nModel Performance with PCA (RMSE):')\n",
    "for name, rmse in results_pca.items():\n",
    "    print(f'{name}: {rmse:.4f}')\n",
    "\n",
    "print('\\nBest Model with PCA:')\n",
    "best_model_pca = best_models_pca[min(results_pca, key=results_pca.get)]\n",
    "print(f'{best_model_pca}: {results_pca[min(results_pca, key=results_pca.get)]:.4f}')\n",
    "\n",
    "print(best_model_pca.get_params())\n",
    "\n",
    "# Visualize explained variance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(explained_variance, marker='o', label=\"Cumulative Explained Variance\")\n",
    "plt.axhline(y=explained_variance[-1], color='r', linestyle='--', label=f\"{explained_variance[-1]:.2f} variance explained\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Explained Variance by PCA Components\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = Ridge()\n",
    "\n",
    "param_grid = {'alpha': [0.1, 1, 10, 100]}\n",
    "\n",
    "sfs = SequentialFeatureSelector(\n",
    "    model,\n",
    "    n_features_to_select='auto',\n",
    "    direction='forward',\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "X_train_sfs = sfs.transform(X_train)\n",
    "X_test_sfs = sfs.transform(X_test)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_sfs, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_rmse = np.sqrt(-grid_search.best_score_)  # RMSE\n",
    "\n",
    "print(f'Best Model: {best_model}')\n",
    "print(f'Best RMSE: {best_rmse:.4f}')\n",
    "print(best_model.get_params())\n",
    "\n",
    "selected_features = X.columns[sfs.get_support()]\n",
    "print(f\"Selected Features for Ridge Model: {selected_features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
